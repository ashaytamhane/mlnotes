{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replication means keeping a copy of the same data on multiple machines\n",
    "  - Can help with keeping data geographically closer\n",
    "  - Allow system to continue working even if some parts have failed\n",
    "  - Scale out number of machines that can serve read queries\n",
    "  - In this chapter, we will assume dataset is small that each machine can hold copy of the dataset. This will be relaxed in chapter 6 where we discuss sharding (partitioning) of big datasets\n",
    "  - If data that we are replicating is same across time, then replication is easy. All the difficulty in replication lies in handling changes to replicated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Three popular algorithms for replicating changes:\n",
    " - Single-leader\n",
    " - Multi-leader\n",
    " - Leaderless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaders and followers\n",
    " * Each node that stores a copy of the db is called a replica\n",
    " * Every write to the db needs to be processed by every replica for consistency\n",
    " * Most common solution for this is leader-based replication (also called active/passive or master/slave replication)\n",
    "   - One of the replicas is designated as leader and others are called followers\n",
    "   - Whenever leader writes new data to local storage, it also sends data change to all of its followers as part of replication log or change stream\n",
    "   - Each follower takes the log from leader and updates local copy by applying same order of writes as in original copy\n",
    "   - When client wants to read from db, it can query either leader of any followers\n",
    "   - Writes are only permitted on the leader\n",
    "   - This mode of operation is built-in feature of many relational dbs like PostgreSQL, MySQL. Also used in some non relational dbs like MongoDB, RethinkDB\n",
    "   - Leader based replication is not restricted to only dbs, but also distributed message brokers like Kafka and RabbitmQ. Some network filesystems and replicated block devices like DRBD are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous vs Asynchronous Replication\n",
    " * Important detail is if replication happens synchronously or asynchronously. In relational dbs, this is often config driven\n",
    " * In synchoronous, leader waits until follower 1 has confirmed that it received write before reporting success to user\n",
    " * In asynchronous, leader sends message without waiting for follower. There are no guarantees on how long this might take\n",
    " * In asynch: if follower is recovering from failure, followers might fall behind leader even by few minutes\n",
    " * In sync: Follower is guaranteed to have up-to-date copy consistent with leader. Even if leader fails, we know there is an up-to-date copy with follower. Disadvantage is that if the synch follower does not respond due to failure or other reasons, write cannot be processed as leader will block writes till synch follower is available\n",
    " * Therefore all followers cannot be in synch in practice as otherwise whole system can grind to halt\n",
    " * In practice, if we enable synch replication on db, it means one of the followers is synch, others are asynch\n",
    " * If the synch follower becomes unavailable/slow, one of the asynch followers is made synch follower. This config is called semi-synch\n",
    " * Often leader based replication is configured to be asynch. In this case if leader fails and not recoverable, then writes that are not replicated to followers are lost and hence write is not durable. On the other hand, leader can continue processing writes even when all followers fall behind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up New Followers\n",
    " * We may need to add more followers from time to time to increase number of replicas or to replace failed nodes\n",
    " * We cannot simply copy files as clients are constantly writing to db and data is in flux. We could lock the db but then availability suffers\n",
    " * Process followed typically is:\n",
    "  - Take a consistent snapshop of leader's db at some point in time, if possible without locking\n",
    "  - Copy snapshot to few follower nodes\n",
    "  - Follower connects to leader and requests changes since snapshot\n",
    "  - When follower processes backlog of data since snapshot, it has \"caught up\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Node Outages\n",
    " * Any node can go down - both unexpected due to fault or expected due to maintenance\n",
    " * Being able to reboot individual nodes without downtime is a big advantage for ops\n",
    " * Therefore goal is to keep system as a whole running despite individual node failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follower Failure: Catch-up recovery\n",
    " * Each follower keeps a log of data changes received from leader\n",
    " * If follower crashes or there are network delays, follower can recover easily from the log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leader Failure: Failover\n",
    " * Handling failure of leader node is trickier: one of the followers need to be promoted to become new leader and followers need to be reconfigured to start consuming from new leade. This is called failover\n",
    " * Failover can happen manually (admin is notified and they take necessary steps to make new leader) or automatically\n",
    " * Automatic failover process:\n",
    "  - Determining that leader has failed: Since this is tricky, most systems just use timeout\n",
    "  - Choosing a new leader: Choose node which has most up-to-date data changes from old leader OR based on previously elected \"controller\" node\n",
    "  - Reconfiguring system to use new leader: Clients send write requests to new leader. If old leader comes back, system needs to ensure it now becomes a follower and recognises new leader\n",
    " * Lot of things can go wrong with failover:\n",
    "  - If async replication is used, new leader may not have received all writes from old leader\n",
    "  - Discarding writes may be dangerous, especially if external db needs to be co-ordinated\n",
    "  - In certain cases, two nodes believe they are the leader (called \"split brain\")\n",
    "  - There is no good timeout to declare leader dead\n",
    "  - There are no easy solutions to these problems. Some operations team therefore prefer to perform failovers manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Replication Logs\n",
    " * Statement based replication: \n",
    "   - In the simplest case every write request (statement) that leader executes is  logged and sent to followers. Example: in relational db, every insert, update, delete is forwarded to followers who parse and execute those SQL statments\n",
    "   - Some issues that may crop up:\n",
    "   - Any statement that calls non deterministic functions (like current date, random) is likely to generate different value on each replica\n",
    "   - If statements use autoincrementing column or if they depend on existing data in db (update x where y) then they must be executed in same order on each replica\n",
    "   - If statements have any side effects (triggers, stored procedures, user defined functions), then those could lead to unpredictable outcomes on each replica\n",
    "   - Since there are many edge cases, typically other replication methods are preferred\n",
    "   \n",
    " * Write-ahead log (WAL) shipping:\n",
    "   - As we saw in chapter 3, storage engines usually append writes to a log\n",
    "   - We can use this log to build replica on another node: besides writing the log to disk, the leader also sends it across the network to its followers\n",
    "   - When the follower processes this log, it builds a copy of exact same data structures as found on the leader\n",
    "   - However, the disadvantage is that running different versions of storage format is not feasible (which is required for zero downtime upgrade of db software)\n",
    "   \n",
    " * Logical (row-based) log replication\n",
    "   - Alternate is to use different log formats for replication and for storage engine, which allows decoupling from storage engine\n",
    "   - This is called logical log\n",
    "   - Logical log for relational db is usually a sequence of records with info at row level\n",
    "   - It is also easier for external applications to parse. This is useful if you want to send contents of db to an external system such as a data warehouse for offline analysis or custom indexes and caches. This technique is called change data capture\n",
    "   \n",
    " * Trigger-based replication\n",
    "   - Replication techniques above are implemented by db system without involving any application code\n",
    "   - While this is good for most cases, there are some cases when more flexibility is required\n",
    "   - Ex: when we want to replicate only a subset of data, or want to replicate from one kind of db to another, or conflict resolution logic\n",
    "   - Trigger lets you register custom application code that is auto executed when a data change occurs in db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Replication Lag\n",
    " - Being able to tolerate node failures is just one reason to want replication. Other reasons are scalability (processing more requests than what single machine can handle) and latency (geo location proximity to user)\n",
    " - Leader-based replication requires all writes to go through single node, but read only can go to any follower\n",
    " - For workloads having mostly reads, we can create many followers and distribute read requests across followers (read-scaling)\n",
    " - This works well realistically only with async replication as with sync config entire system could be unavailable for writing if single node fails\n",
    " - Even with asynch replication, info might be outdated if follower has fallen behind and can lead to inconsistency in db\n",
    " - This inconsistency is temporary as eventually follower will cath up (called eventual consistency)\n",
    " - Replication lag is delay happening between write happening on leader reflecting on follower\n",
    " - Typically this lag is only fraction of second, however if system is operating near capacity this can go to minutes\n",
    " - Three problems that are likley to occur with replication lag\n",
    "   * Reading Your Own Writes: reading data immediately after writes may have issues due to lag (ex: users viewing what data they have submitted on website)\n",
    "   * Monotonic reads: If user makes several reads from different replicas, it can happen that user sees things moving backward in time (due to lag). Monotonic reads is guarantee that this does not happen (one way to do this is by making sure user reads from same replica)\n",
    "   * Consistent Prefix reads: Sequence of events/writes should be maintained. Such guarantee is called consistent prefix reads. This is typically a problem with partitioned dbs (one way to solve this is by making sure all casually related data are written to same partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Leader Replication\n",
    " * Single leader has a major downside: all writes must go through the single leader creating single point of failure\n",
    " * Natural extension is to allow more than one node to accept writes\n",
    " * Replication happens in same way: each node that processes a write must forward data change to all nodes (multi-leader config or master-master or active/active replication)\n",
    " * Each leader simultaneously acts as a follower to other leaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use-cases for Multi-leader Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multi-Datacenter operation\n",
    " - Consider a db with replicas in several different datacenters (could be for both handling failures+being closer to users)\n",
    " - In normal leader-based replication, leader has to be in one of these datacenters and all writes must go through it\n",
    " - In multi leader config, we can have a leader in each datacenter \n",
    " - Within each datacenter regular leader-follower replication is used and between datacenters each leader replicates to other leaders\n",
    " \n",
    "* Performance: Since every write can be processed in the local datacenter, the perceived performance may be better\n",
    "* Tolerance of datacenter outages: Each datacenter can continue operating independently\n",
    "* Tolerance of network problems: Multi leader with asynch config is better at handling unreliable network issues across data centers\n",
    "* While multi leader has advantages, it also has big downside: the data may be concurrently modified into two datacenters and those write conflicts must be resolved\n",
    "* Since multi leader replication is retrofitted feature in many dbs, there are often subtle config pitfalls (autoincrementing keys, triggers can be problematic) and hence multi-leader config is considered dangerous and avoided if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clients with offline operation\n",
    " * Another use case for multi-leader replication is if application needs to continue to work when disconnected from internet\n",
    " * This is a multi leader replication between datacenters where each device becomes a datacenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Editing\n",
    " * Google docs and other real-time collaborative editing applications allow several people to edit concurrently\n",
    " * When one user edits, changes are instantly applied to local replica and async replicated to other users\n",
    " * To guarantee there are no editing conficts, application must obtain a lock on the document. In this case, its equivalent to single leader replication\n",
    " * However for faster collaboration, we can make unit of change very small (keystroke) and avoid locking - this will require to handle all challenges of multi leader replication including conflict resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Write Conflicts\n",
    " * Biggest problem with multi-leader replication is write conflicts and resolution required\n",
    " * Example: Wiki page editing done simultaneously by multiple users. Each user's change is applied to local leader, however when changes are asynch replicated conflict is detected (this does not occur in single leader replication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conflict avoidance\n",
    " * Since many implementations of multi-leader replication handle conflicts poorly, its better to avoid conflicts\n",
    " * If the application can ensure that all writes for particular record go through same leader, then conflicts cannot occur\n",
    " * Example: if user is editing their own data, we can ensure requests from a particular user are always routed to same datacenter and use the leader in that datacenter for reading/writing\n",
    " * There may still be an issue when one datacenter fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converging to consistent state\n",
    " * When all changes have been replicated, all replicas should have exact same final value for all fields\n",
    " * Ways to achieve this:\n",
    "   - Give each write a unique ID (timestamp or random variable), pick the write with highest ID and discard other writes. This could lead to data loss\n",
    "   - Give each replica an ID and let writes originated at higher ID to take precedence (also leads to data loss)\n",
    "   - Merge the conflicting values together somehow (example: order them alphabetically and concatenate them)\n",
    "   - Record the conflict in explicit data structure that preserves all information and write application code to resolve them at later time (maybe by prompting the user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Conflict resolution logic\n",
    " * As most appropriate way might depend on application, most multi-leader replication tools allow us to write conflict resolution using application code which may be executed on read or write\n",
    " * Some approaches for automatic conflict resolution\n",
    "   - Conflict-free replicated datatypes (CRDTs): family of data structures for sets, maps, ordered lists, counters that can be concurrently edited by multiple users which automatically resolve conflicts\n",
    "   - Mergeable persistent data structures: track history explicitly, similar to git version control and use a three way merge function (CRDTs use two way merge)\n",
    "   - Operational transformation is the conflict resolution algo used in Etherpad and Google Docs. It was designed for concurrent editing of ordered list of items, such as the list of characters in document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Leader Replication Topologies\n",
    "  * Replication topology describes communication paths along which writes are propagated from one node to other\n",
    "  * When there are only two leaders, only one topology exists (each leader sends writes to each other)\n",
    "  * However, when more than 2 leaders exist, multiple topologies are possible\n",
    "    - Circular, Star, All-to-all\n",
    "    - Causal ordering is an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaderless Replication\n",
    " * Some data storage systems do not use concept of leaders and allow direct writes on any replica\n",
    " * Dynamo from Amazon uses leaderless replication\n",
    " * In some leaderless implementations, client directly sends writes to several replicas while in others a coordinator node does this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to the db when Node is down\n",
    " * In leaderless config, failover does not exist\n",
    " * Even if one of the replicas did not get successful write (say 2 out of 3 replicas were successful), client simply ignores this\n",
    " * Since this replica missed the write, it might return outdated data as response\n",
    " * To handle this, client sends read requests to several nodes in parallel and chooses one which is most updated (using versioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read repair and anti-entropy\n",
    " * After an unavailable node comes back online, we should ensure that its updated with latest writes\n",
    " * Two mechamisms used in Dynamo-styled datastores:\n",
    "   - Read repair: When client makes read from several nodes in parallel, it can detect node with stale data and update it. This works well for frequently read values\n",
    "   - Anti-entropy process: In addition, some datastores have a background process that constantly looks for differences in data between replicas and copies any missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quorums for reading and writing\n",
    " * Earlier, we assumed 2 out of 3 as the threshold for marking success\n",
    " * More generally, if there are n replicas, every write must be confirmed by w nodes to be considered succesful and we must query atleast r nodes for each read\n",
    " * As long as w+r>n, we can expect up to date value while reading because atleast one of the r nodes nodes will be up to date\n",
    " * Edge cases:\n",
    "   - If a sloppy quorum is used, w writes may end up on different nodes than r reads\n",
    "   - If two writes happen concurrently, it is not clear which one happened first. In this case, need to merge the two\n",
    "   - If a write happens concurrently with a read, the write may be reflected on only few replicas\n",
    "   - If a write succeeded on some replicas and failed on others and overall succeeded on less than w replicas, it is not rolled back on replicas where it succeeded\n",
    "   - If a node carrying a new value fails, and its data is restored from replica with stale value, quorum condition fails\n",
    " * Stronger guarantees on avoiding problems with replication lag and quorum need transactions or consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring staleness\n",
    " * Need to keep proactively measuring staleness of data across the db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sloppy Quorums and Hinted Handoff\n",
    " * A network interruption can cut off a client from large number of db nodes. Fewer than w or r reachable nodes might remain, so client cannot reach quorum\n",
    " * In a large cluster, client can connect to some nodes to assemble quorum for a value (not necessarily nodes that it needs for quorum)\n",
    "  - Is it betterto return errors for all requests for which we cannot reach quorum\n",
    "  - Should we accept writes anyway, and write them to some nodes that are reachable, but not necessarily the \"home nodes\" for the value\n",
    " * The later case is called as sloppy quorum\n",
    " * Once the network interruption is fixed any writes that one node temporarily accepted on behalf of other node is sent to the right home node. This is called  hinted handoff\n",
    " * Sloppy quorums are useful for increasing write availability. They are optional in all Dynamo implementations\n",
    " * In Riak, they are enabled by default. In Cassandra and Voldemort they are disabled by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

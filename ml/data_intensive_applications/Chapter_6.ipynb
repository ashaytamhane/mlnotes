{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For large datasets or high throughput, replication is not sufficient. We need to break data up into partitions also known as sharding\n",
    "* Paritioning is called shard in MongoDB, Elasticsearch; region in HBase; tablet in Bigtable, vnode in Cassandra and Riak\n",
    "* Normally, partitions are defined in a way that each piece of data (each record, row or document) belongs to exactly one partition\n",
    "* Main reason for patitioning is scalability - different partitions can be placed on different nodes of shared nothing cluster\n",
    "* Query load therefore gets distributed across many processors\n",
    "* Queries that operate on single partition can execute independently, other complex queries can be potentially parallelised across nodes (much harder)\n",
    "* In this chapter, we will look at diferent approaches for partitioning large datasets, observe impact of indexing and look at rebalancing which is necessary if we want to add/remove nodes in cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning and Replication\n",
    " * Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes\n",
    " * Even if record belongs to one partition, it may still be stored on several nodes\n",
    " * Assuming leader-follower model for replication:\n",
    "   - Each partition's leader is assigned to one node and follower assigned to other nodes\n",
    "   - Each node may be leader for some partitions and follower for others\n",
    "   - Everything from chapter 5 on replication applies to replication of partitions\n",
    "   - Choice of partitioning scheme is mostly independent of replication scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning of Key-Value Data\n",
    " * How to decide which records to store on which nodes?\n",
    "   - Goal with partitioning is to spread data and query load evenly across nodes\n",
    "   - In a \"fair\" partitioning, this load is spread evenly across nodes. If it is \"unfair\", the data will get skewed and some partitions might have more data or queries than others (which is not good)\n",
    "   - A partition with disproportionate load is called \"hot spot\"\n",
    "   \n",
    " * Avoiding hotspots\n",
    "   - Simplest approach to avoid hotspot is to assign records randomly to nodes for even distribution\n",
    "   - However big disadvantage is that we have no way to know which node a particular item is on and need to query all nodes in parallel\n",
    "   - Better techniques discussed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning by Key Range\n",
    " - One way to partition is by assigning a continuous range of keys\n",
    " - If we know boundries between ranges, we can easily determine which partition contains a given key\n",
    " - Ranges of keys are not necessarily evenly spaced because data may not be evenly distributed. To evenly distribute data, boundries need to adapt to data\n",
    " - This partitioning strategy is used by Bigtable, HBase, RethinkDB, earlier versions of MongoDB (<2.4)\n",
    " - Within each partition, we can keep keys in sorted order (SSTables/LSM-Trees)\n",
    " - This is especially useful if we want range queries (ex: all readings from a particular month)\n",
    " - Downside of key range is that certain access patterns can lead to hot spots. Example: If timestamp is key (sensor data), then all writes will go to same partition (since timestamp range is same for the day) and rest of the partitions sit idle. To avoid this, we can use a prefix for each timestamp (say sensor name) so that partitioning first happens by sensor name and then by time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitoning by Hash of Key\n",
    " * Because of risk of hotspots, many distributed datastores use hash function to determine partition for given key\n",
    " * A good hash function will evenly distribute the data. Even if the input strings are similar, their hashes are evenly distributed across range of numbers\n",
    " * For partitioning purpose, hash function need not be cryptographically strong. Ex: Cassandra and MongoDB use MD5. Many programming languages have simple hash functions inbuilt , but they may not be suitable for partitioning. In JAVA's Object.hashcode(), same key may have different hash value in different processes\n",
    " * Once we have hash function for keys, we can assign each partition a range of hashes\n",
    " * This technique is good at distributing keys fairly among partitions\n",
    " * The partition boundries can be evenly spaced or choosen pseudorandomly (called consistent hashing)\n",
    " * Using hash of key for partitioning we loose the ability to do efficient range queries as adjacent keys are scattered across all partitions and sort order is lost. Ex: In MongoDB, if we enable hash-based sharding, any range query has to be sent to all partitions. Range queries on primary key are not supported by Riak, Couchbase and Voldemort\n",
    " * Cassandra allows table declaration with a compound primary key of several columns. First part of the key is used for hashing, other columns are used as concatenated index for sorting data in SSTables. Therefore query cannot search for range of values on first column, but given a fixed value for first column can perform efficient range queries on rest of the columns of key. Ex: if primary key is (user_id, timestamp) then data of different users can reside on different partitions, however for each user the data can be sorted by timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed Workloads and Relieving Hotspots\n",
    " * In some cases, some keys can be hot (ex: celebrity user ids)\n",
    " * Although this is not a solved problem completely, one could add random numbers in the user key\n",
    " * This however needs additional book keeping and need to read data from all the keys related to user id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning and Secondary Indexes\n",
    " * So far we have discussed key-value data model. If records are mostly accessed by primary key, we can determine partition from key \n",
    " * It gets more complicated if secondary indexes are involved. Secondary index is not for uniquely identifying a record, but for searching values (ex: all cars that are red in colour)\n",
    " * Problem with secondary indexes is that they don't map neatly to partitions\n",
    " * Two main approaches to partitioning a db with secondary indexes\n",
    "   - Document-based partitioning\n",
    "   - Term-based partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document based Partitioning\n",
    " * Assume a website for selling used cars. Each listing has a unique id called document id. We partition db by the document id\n",
    " * Since we want users to search cars and filter by colour/make (in a document db these would be fields, in a relational db they would be columns), we need to declare secondary index on colour/make\n",
    " * If we declare the index, db can perform indexing automatically - db paritition automatically can add it to list of document ids for index entry colour:red\n",
    " * Each partition is completely separate: it maintains its own secondary index covering only documents in that partition. This is why its also called local index\n",
    " * However, note that cars from a specific colour can be spread acrossy partitions. Therefore, if we want to search for red cars, we need to query all partitions and combine results. This approach is at times called scatter/gather and can make queries on secondary index expensive\n",
    " * Nonetheless it is still used widely: MongoDB, Riak, Cassandra, ElasticSearch, SolrCloud, VoltDB all use document partitioned secondary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term-based partitioning\n",
    " * Instead of each paritition having its own secondary index (local index), we can construct a global index that covers data from all partitions\n",
    " * This global index must be partitioned and stored across all nodes (all nodes would need access to it). If we store it on only one node, it will become a bottleneck\n",
    " * It is called term partitioning as the term we are looking for (ex: colour:red) determines the partition of the index\n",
    " * As usual, we can partition by using the term itself or using hash of the term. Partitioning by term is useful for range scans (price between x and y) where as partitioning on hash ensures a more even distribution of load\n",
    " * Advantage of global term partitioning over local document partitioned index: rather than doing scatter/gather over all partitions, a client only needs to make request to the parition containing the term. \n",
    " * Downside is that writes are slower and more complex, because write to single document may affect multiple partitions of index\n",
    " * In practice, updates to global secondary indexes are often asynch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebalancing partitions\n",
    " * Over time things change in db:\n",
    "   - Query throughput increases, so we may like to add more CPUs to handle load\n",
    "   - Dataset size increases, so we may like to add more RAM\n",
    "   - Machine fails and other machines need to take over\n",
    " * All of these changes call for data and requests to be moved from one node to other. This process of moving load from one node in cluster to another is called rebalancing\n",
    " * Irrespective of rebalancing scheme, some minimum requirements should be met:\n",
    "   - After rebelancing, load should be shared fairly between nodes\n",
    "   - While relebancing is happening, db should continue reads/writes\n",
    "   - Minimise network and disk I/O by limiting data movement between nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies for Rebalancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How not to do it: hash mod N\n",
    " * If we use mod function to rebalance, it will cause a lot of movement of data across nodes and hence is not recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed number of partitions\n",
    " * A simple solution is to create many more partitions than number of nodes and assign several partitions to each node\n",
    " * If a new node is added to cluster, it can \"steal\" a few partitions from existing nodes until partitions are fairly distributed again\n",
    " * Number of partitions and assignment of keys to partitions remains same. Only assignment of partition to nodes changes\n",
    " * This change of assignment takes time to transfer large amount of data over network\n",
    " * Old assignment is used for reads/writes while transfer is in progress\n",
    " * We can account for mismatched hardware in cluster by assigning more partitions to powerful nodes\n",
    " * This approach to partitioning is used in Riak, ElasticSearch, Couchbase and Voldemort\n",
    " * Choosing the right number of partitions is difficult if total size is highly variable\n",
    " * If partitions are very large, rebalancing and recovery becomes expensive. If partitions are too small, they incur too much overhead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic partitioning\n",
    " * For Dbs using key range partitioning, fixed number of partitions with fixed boundries would be inconvenient (if you got boundries wrong, you could end up with skew). This is why key range based dbs like HBase and RethinkDB create partitions dynamically\n",
    " * When partition grows beyond a configured size (10 gb in HBase), it is split into two partitions that share roughly half of the data\n",
    " * Conversely, if partition shrinks belows some threshold, it can be merged with an adjacent parent. This process is similar to top level of B-tree\n",
    " * Each node can handle multiple partitions - after a large partition has been split, one of the halves. In case of HBase, transfer happens via HDFS (underlying distributed file system)\n",
    " * Advantage is that number of partitions adapts to total data volume\n",
    " * Empty database has only one partition. While db is small it keeps hitting same parition. To deal with this, HBase and MongoDB allow initial number of partitions to be configured\n",
    " * Dynamic partitioning can also be used with hash paritioned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning proportionally to nodes\n",
    " * With both dynamic partitioning, number of partitions is proportional to size of data. In fixed partitioning, size of each partition is proportional to size of dataset. In both of these cases, number of partitions is independent of number of nodes\n",
    " * Third option used by Cassandra and Ketama is to make number of partitions proportional to number of nodes (fixed set of partitions per node)\n",
    " * When new node joins the cluster, it randomly chooses a fixed number of existing partitions to split and takes ownership of half of split partitions\n",
    " * Picking partition boundaries randomly requires hash partitioning. This approach is called consistent hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations: Automatic or Manual Rebalancing\n",
    " * It is helpful to have human in the loop for rebalancing as there are cases where automation can be dangerous\n",
    " * One example: If a node is overloaded and temporarily slow to respond to requests, other nodes may assume this node is dead and start automatically rebalancing. This may put additional load on overloaded node and other nodes leading to cascading failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Routing\n",
    " * When a client mants to make a request, it should know which node to connect to\n",
    " * Since partitions are rebalanced, there has to be layer on top to serve requests\n",
    " * This is a more general problem called service discovery (applicable to all software, not just databases)\n",
    " * Different approaches:\n",
    "   - Allow clients to contact any node (say via round-robin load balancer). If that node owns partition, it can handle requests directly else redirect request to appropriate node, gets response and passes reply to client\n",
    "   - Send all requests to routing tier first, which determines which node should handle the request and forwards request accordingly\n",
    "   - Clients can be aware of partitioning and assignment of nodes. They can therefore connect directly to appropriate node\n",
    " * In all the cases, it is important for component making routing decision to be aware of assignment of partitions to node. This is a challenging problem as all participants should agree and driving consensus in distributed systems is hard\n",
    " * Many distributed sytems rely on separate coordination service like zoo-keeper to track of cluster metadata\n",
    " * Zookeeper maintains mapping of partitions to nodes. Other actors like routier tier or partitioning aware client can subscribe to this info in zookeeper\n",
    " * Linkedin's Espresso uses Helix for cluster management (which in turn relies on zookeeper), HBase, SolrCloud, Kafka also use zookeeper to track partition assignment. MongoDB also has similar architecture, but has its own config server implementation and mongos daemons as routing tier. Cassandra and Riak use \"gossip protocol\" among nodes to disseminate changes in cluster state. Requests can be sent to any node and it forwards request. Couchbase does not rebalance automatically and is configured with routing tier called moxi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
